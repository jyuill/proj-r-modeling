---
title: "Tidy-models-Datacamp-1"
author: "John Yuill"
date: "`r Sys.Date()"
output: 
  html_document:
      code_folding: show

---

# DataCamp course on Tidy Models pkg {.tabset}

[Course](https://campus.datacamp.com/courses/modeling-with-tidymodels-in-r)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(lubridate)
library(scales)
library(here)
library(ISLR) ## for logistic regression sample data

options(scipen = 10)
```

## Linear Regression Model

Predicting numerical value of outcome variable.

### Steps:

1. Get Data
2. Split into training / test: initial_split(data, prop, strata))
3. Specify model parameters: <model type>() %>% set_engine() %>% set_mode()
4. Fit model: <model params> %>% fit(dependent ~ independent, data)
5. Generate predictions with test set to evaluate: predict(<fit model>, new_data)
6. Bind prediction with test data
7. Evaluate accuracy with rmse: combined test data %>% rmse(truth, estimate)
8. Visualize accuracy for check
9. Modify model inputs as needed to try to improve accuracy

### Get Data

Requirements for predicting numerical outcome: 

* numerical independent/predictor values
* numerical dependent/outcome variable
* individual cases with multiple instances of each variable, as opposed to just aggrgated data

Use **soc_search_vid_unit_model_ready.csv** since readily available.

```{r}
mdata <- read_csv(here('data','soc_search_vid_unit_model_ready.csv'))
head(mdata)

```

### Split into Training and Test sets

* uses functions from rsample pkg
* initial_split function to set up splitting parameters
* prop = % split between two groups (usually set for training group proportion)
* strata = outcome variable, to ensure that each group has equal % representation of the outcome variable

```{r}
## set up splitting function
mdata_split <- initial_split(mdata,
                             prop=0.8,
                             strata = units)
## get training
mdata_train <- mdata_split %>% training
## get test
mdata_test <- mdata_split %>% testing

```

### Linear Regression

* functions from parsnip pkg
* model object: set up model parameters: model function, set_engine, set_mode
* fit object: apply model parameters to fit a model based on fit values
* tidy(fit object) to get summary output

#### With Convo Vol

```{r}
## set up model parameters
lr_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode('regression')

lr_fit <- lr_model %>%
  fit(units ~ convo_vol, data=mdata_train)

print(lr_fit)
tidy(lr_fit)
  
```

#### With Google Search Index

```{r}
lr_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode('regression')

lr_fit <- lr_model %>%
  fit(units ~ index, data=mdata_train)

print(lr_fit)
tidy(lr_fit)
  
```

#### With both

```{r}
lr_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode('regression')

lr_fit <- lr_model %>%
  fit(units ~ convo_vol+index, data=mdata_train)

print(lr_fit)
tidy(lr_fit)
  
```

### Evaluate prediction

* use test data to make predicts based on model and evaluate accuracy
* generate prediction using test data
* bind predictions to test data to compare against actuals
* yardstick pkg: evaluate performance of predictions on test data using functions

#### Generate predictions

* get predictions and combine with actuals

```{r}
unit_predict <- predict(lr_fit, new_data=mdata_test)

unit_test <- mdata_test %>% select(convo_vol, index, units) %>% bind_cols(unit_predict)

head(unit_test)

```

#### Evaluate accuracy

* yardstick pkg: evaluate performance based on accuracy
* rmse
* rsq

```{r}
## rmse
unit_test %>% rmse(truth=units, estimate=.pred)
# rsq
unit_test %>% rsq(truth=units, estimate=.pred)
```

#### Visualize Performance

```{r}
unit_test %>% ggplot(aes(x=units, y=.pred))+geom_point()+
  geom_abline()+ ## diagonal line
  coord_obs_pred() ## scale x, y axes to match
```

## Classification: Logistic Regression

Numerical and/or categorical data with binay outcome variable.

### Steps:

1. Get Data
2. Split into training / test: initial_split(data, prop, strata))
3. Specify model parameters: <model type>() %>% set_engine() %>% set_mode()
4. Fit model: <model params> %>% fit(dependent ~ independent, data)
5. Generate predictions with test set to evaluate: predict(<fit model>, new_data)
6. Bind prediction with test data
7. Evaluate accuracy:
    + confusion matrix
    + sensitivity vs specificity
    + other evaluation metrics
    + ROC AUC

### Get Data

* from ISLR pkg - credit card default data

```{r}
ccdata <- Default
head(ccdata)

str(ccdata)
```

Note that the outcome variable - default - is a binary variable, with class = factor and factors in order 'No' and 'Yes'. This will mess up modelling because will assume that first factor = positive outcome. 

Need to relevel to avoid confusion in the confusion matrix.

```{r}
## need to relevel outcome variable to have 'positive' outcome first
## - this will flow through to training and test data sets and for both actual and ## predicted classification
ccdata$default <- fct_relevel(ccdata$default, "Yes","No")
## confirm
str(ccdata)
```


### Split to Training / Test

```{r}
set.seed(2021) ## set seed to preserve same test / training if recreated later
cc_split <- initial_split(data=ccdata, prop=0.75, strata=default)
cc_training <- cc_split %>% training
cc_test <- cc_split %>% testing

nrow(cc_training)
nrow(cc_test)

## check distribution of default / not default for training and test set
table(cc_training$default)
table(cc_test$default)
```


### Logistic Regression

Create model on training set.

```{r}
## function from parsnip
## specify the model
logr_model <- logistic_reg() %>%
  ## set engine
  set_engine('glm') %>%
  ## set mode
  set_mode('classification')

## fit model to training data - numerical data only
logr_fit <- logr_model %>% fit(default ~ balance + income, data=cc_training)

## Print model
logr_fit

```

### Generate Predictions

* run prediction on test set
* produces data frame with 1 var: .pred_class (since classification model) 
* bind predictions to test set for comparison with actual classifications

```{r}

default_predict_class <- predict(logr_fit, new_data=cc_test, type='class')

default_test <- cc_test %>% bind_cols(default_predict_class)

head(default_test)

```

```{r}

default_predict_prob <- predict(logr_fit, new_data=cc_test, type='prob')

default_test <- default_test %>% bind_cols(default_predict_prob)

head(default_test)
```

### Evaluate  Model

Evaluate predictions.

* Functions from **Yardstick** pkg
* consistent values/structure for all the functions showcased below

#### Confusion Matrix

* Specify data set, 'truth' column, 'estimate' column

```{r}
## get confusion matrix
confusion <- conf_mat(default_test, 
                      truth=default, 
                      estimate=.pred_class)
confusion

no_percent <- sum(confusion$table[,2])/sum(confusion$table)
```

#### Check Accuracy

* accuracy functio same values/structure as conf_matrix - easy!
* calculated as % correct: true positives and true negatives divided by all
    + (TP+TN)/(TP+FP+TN+FN)
* generally NOT the best metric - esp if inbalance, predicting dominant class for all will yield high accuracy.
* above confusion matrix shows 'No' is `r no_percent*100` % of total, so just predicting ALL as No will give `r no_percent*100` % accuracy. 

```{r}
accuracy(default_test, 
         truth=default, 
         estimate=.pred_class)
```

* better than just predicting all as 'No' in this case, but more nuanced evaluation is useful

#### Sensitivity

* Sensitivity evaluates how many Positives ('Yes' in this case) were correctly identified
* Improves with True Positive Rate (prediced positive when actually positive)
* Low sensitivity means tendency to under-predict positives
* sens function same values/structure as conf_matrix & accuracy - easy!

```{r}
sens(default_test, truth=default, estimate=.pred_class)
```

* Sensitivity is LOW so the model is missing lots of Positives and classifying incorrectly as Negative
* Very conservative model, unable to confidently differentiate positives from negatives, reluctant to over-commit to positives

#### Specificity

* Specificity evaluates how many Negatives ('No' in this case) were correctly identified
* Improves with True Negative Rate (predicted negative when actually negative)
* Low specificity means tendency to over-predict positives, generating False Positives
* False Positive Rate = 1-Specificity
* spec function same values/structure as conf_matrix & accuracy & sens - easy!

```{r}
spec(default_test, truth=default, estimate=.pred_class)
```

* Very low False Positive Rate
* Erring on the side of negatives, not wanting to make mistakes on the positive side
* Could be good in situations where it is more desirable to catch the positives that you are sure of and ok with letting some positives slip through, rather than falsely declaring positives
    + when limited resources to act on positives and want to have high confidence that the ones acting on are actually positive
    + stakes of letting positives through is not high (marketing re-targeting vs deadly disease) 
    
#### Combine Accuracy, Sensitivity, Specificity

* Custom metric by using metric_set() function in Yardstick
* metric_set function same values/structure as conf_matrix & accuracy & sens & spec - easy!

```{r}
custom_metric_combo <- metric_set(accuracy, sens, spec)
custom_metric_combo(default_test, truth=default, estimate=.pred_class)
```

* Tells combined story on model accuracy/effectiveness in one place
* Good or Bad? Depends on purpose
* Accuracy is high - but needs to be compared to blanket classification
* Sensitivity is low - depends how important it is to catch ALL the positives
    + may want a model that has higher likelihood of correctly classifying true positives
* Specificity is high - depends on tolerance for false positives
    + may want model that is not as strict and accepts more mis-classification of positives - IF also improves sensitivity to true positives
    
#### ALL metrics

* ALL the Binary Classification metrics within Yardstick
* conf_mat > summary
* [Yardstick reference]('https://yardstick.tidymodels.org/reference/') - for more info

```{r}
conf_mat(default_test, truth=default, estimate=.pred_class) %>%
  summary()
```

#### Visualize Model Results

* Heatmap

```{r}
conf_mat(default_test, truth=default, estimate=.pred_class) %>%
  autoplot(type='heatmap')
```

* mosaic: balances columns based on proportions

```{r}
conf_mat(default_test, truth=default, estimate=.pred_class) %>%
  autoplot(type='mosaic')
```

#### ROC Curve!

* Recieving Operating Characteristic (ROC) Curve
* Visualize performance across probability thresholds
* Sensitivity (Y-axis) vs 1-Specificity (X-axis)
* Optimal: 0,1
* Diagonal is poor performance -> random 50/50

```{r}
default_test %>% roc_curve(truth=default, .pred_Yes) %>% autoplot()
```

#### ROC AUC

* calculate area under ROC curve
* gives overall performance and enables comparison to alternate models

```{r}
default_test %>% roc_auc(truth=default, .pred_Yes)
```

* Seems like very high AUC - I guess function largely of low false positive rate (1-specificity)

## Automation

Use last_fit to combine model fit and test in one step.

### Fit model and evaluate

```{r}
## need model params and split object
# logr_model
# cc_split

cc_last_fit <- logr_model %>% 
  last_fit(default ~ student+balance+income, split=cc_split)

cc_last_fit %>% collect_metrics()

```
### Customize metrics

```{r}
cc_last_fit_results <- cc_last_fit %>% collect_predictions()
cc_last_fit_results

## check confusion matrix - make sure factor levels are in order with 'positive' first
conf_mat(cc_last_fit_results, 
                      truth=default, 
                      estimate=.pred_class)

## select custom metrics of interest
cc_last_fit_metrics <- metric_set(accuracy,
                                  sens,
                                  spec,
                                  roc_auc)
## produce custom metrics - should match results in original log reg above
cc_last_fit_metrics(cc_last_fit_results,
                    truth=default,
                    estimate=.pred_class,
                    .pred_Yes)

#sens(cc_last_fit_results, truth=default, estimate=.pred_class)
#roc_auc(cc_last_fit_results, truth=default, .pred_Yes)

```

## Feat. Eng.

Feature Engineering using recipes pkg via tidymodels (back to original credit card data (ccdata))

* set up the model: logistic regression in this case


### Numerical Data

(sidebar: adding a var to demonstrate how to handle multicolinearity)

```{r}
## add col based on calc from existing col to demonstrate how to handle multi-collinearity
ccdata2 <- ccdata
## create new var with controlled random factor of existing var
for(f in 1:nrow(ccdata2)){
  ccdata2$fakevar[f] <- ccdata2$balance[f]*rnorm(1, mean=12, sd=2.5)
}
## looking for correlation coefficient 0.9-0.98
cor(ccdata2$balance, ccdata2$fakevar)

## split new data to training and test
cc2_split <- initial_split(data=ccdata2, prop=0.8, strata=default)
cc2_train <- cc2_split %>% training()
cc2_test <- cc2_split %>% testing()

```

Create recipe that prepares data for modeling from original ingredients.

* step_corr: identify and remove highly correlated variables, based on threshold
* step_normalize: rescale data by value-mean/sd
* step_range also demonstrated: rescale variables within specified range
* 'bake' the prepared data set

```{r}
cc2_recipe <- recipe(default ~.,
                     data=cc2_train) %>%
  step_corr(all_numeric(), threshold = 0.9) %>%
  step_normalize(all_numeric())

cc2_recipe

cc2_recipe %>% prep(training=cc2_train) %>% bake(new_data=NULL)

## can also use step_range with min and max if you want to normalize variables
##  within a specified range (0-100 in this case):
cc2_rec_rng <- recipe(default ~ ., data=cc2_train) %>%
  step_range(all_numeric(), min=0, max=100)

cc2_rec_rng %>%
  prep(training=cc2_train) %>%
  bake(new_data=NULL)
```

* 'balance' col is removed due to multicolinearity
* remaining cols are normalized (value-mean/sd)

Cool but don't know how to control/influence which columns remain when multi-colinearity is detected. Prefer to keep 'balance' rather than 'fakevar'.

(going back to original ccdata)

* similar preparation according to recipe
* use the recipe to 'bake' the test data into same preparation (new_data=cc_test)

```{r}
## create recipe with prep steps
cc_recipe <- recipe(default ~.,
                     data=cc_training) %>%
  step_corr(all_numeric(), threshold = 0.9) %>%
  step_normalize(all_numeric())
## check recipe
cc_recipe
## apply recipe to training data
cc_recipe_prep <- cc_recipe %>% prep(training=cc_training)

## apply to test data
cc_recipe_prep %>% bake(new_data=cc_test)

```

Can now evaluate the model on the test data, which is in the same form of the training data, thanks to the same recipe being baked.

(detour to work on categorical data before getting back to modeling and evaluation)

### Categorical Data

aka 'Nominal' data

* similar workflow as with numerical data
* convert nominal (categorical) data into numerical, based on whether or not a record is associated with each category item

Create dummy variables

* 'student' is nominal variable, currently expressed as 'Yes' or 'No'
* step_dummy converts to dummy variable, where student_Yes is either 0 or 1
* always 1-less dummy variables than values for the nominal variable, since by definition the value of the remaining (dropped) nominal variable will be dictated by the other variables
    + if student_Yes = 1, then by definition student_No = 0 (and vice versa) so no need to include in model

```{r}
## create recipe with prep steps
cc_recipe <- recipe(default ~.,
                     data=cc_training) %>%
  step_dummy(all_nominal(), -all_outcomes()) ## converts all nominal variables in data set to dummy variables, except outcome variable

## check recipe
cc_recipe
## apply recipe to training data
cc_recipe_prep <- cc_recipe %>% prep(training=cc_training)

## apply to test data
cc_recipe_prep %>% bake(new_data=cc_test)

## COMBINE recipe steps into single process
recipe(default ~ ., data=cc_training) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  prep(training=cc_training) %>%
  bake(new_data=cc_test)

```

## Workflow I

Combining everything covered so far into complete Classification Model workflow.

### Summary steps

1. Split data into training / test
2. Model specification
3. Feature engineering (independent of model spec - could be done before)
4. Model fitting
5. Model prediction
6. Model evaluation

### 1. Split data into training / test

```{r}
set.seed(2021) ## set seed same as earlier to generate same training / test data
cc_split <- initial_split(ccdata, prop=0.75, strat=default)
cc_train_cat <- cc_split %>% training()
cc_test_cat <- cc_split %>% testing()

```

```{r, echo=FALSE}
## check data - compare to original above
cat('New training and test data for categorical variable inclusion: ')
nrow(cc_train_cat)
nrow(cc_test_cat)
table(cc_train_cat$default)
table(cc_test_cat$default)

cat('Original training and test data for first model: ')
nrow(cc_training)
nrow(cc_test)
table(cc_training$default)
table(cc_test$default)
```


### 2. Model Specification

* logistic regression for categorical outcome (default / not default)

```{r}
## specify the model type
cc_logreg_model <- logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')
```

### 3. Feature Engineering 

#### Recipe

* resolve multi-collinearity by removing redundant variables
* normalize numeric variables
* create dummy variables for nominal
* bake the recipe based on above

```{r}
cc_logreg_recipe <- recipe(default ~ ., data=cc_train_cat) %>%
  step_corr(all_numeric()) %>% ## remove correlated numeric variables
  step_normalize(all_numeric()) %>% ## normalize numeric variables
  step_dummy(all_nominal(), -all_outcomes()) %>% ## convert nominal to dummy vars 
  prep(training = cc_train_cat)
cc_logreg_recipe
  
## bake training data with recipe for modeling
cc_train_prep <- cc_logreg_recipe %>%
  bake(new_data = NULL)
cc_train_prep

## bake test data with recipe for model evaluation
cc_test_prep <- cc_logreg_recipe %>%
  bake(new_data = cc_test_cat)
cc_test_prep

```

### 4. Model Fitting

```{r}
cc_logreg_fit <- cc_logreg_model %>% 
  fit(default ~ ., data=cc_train_prep)
## check
cc_logreg_fit
```

### 5. Model Prediction

```{r}
## use fitted model to predict class (default or not default) on test data
cc_logreg_pred_class <- predict(cc_logreg_fit, 
                          new_data=cc_test_prep,
                          type='class')
## use fitted model to predict probabilities of default or not default on test data
cc_logreg_pred_prob <- predict(cc_logreg_fit, 
                          new_data=cc_test_prep,
                          type='prob')
  
```

### 6. Model Evaluation

* confusion matrix

```{r}
## combine test data actual and predicted into same table
cc_logreg_results <- cc_test_cat %>% select(default) %>%
  bind_cols(cc_logreg_pred_class, cc_logreg_pred_prob)
## check
head(cc_logreg_results)

## get confusion matrix - how do we get this by %?
cc_logreg_results %>%
  conf_mat(truth=default, estimate=.pred_class) 

```
```{r}
custom_eval_metrics <- metric_set(accuracy, sens, spec)
custom_eval_metrics(cc_logreg_results, truth=default, estimate=.pred_class)
```
### Compare to initial logistic regression model

```{r}
custom_metric_combo <- metric_set(accuracy, sens, spec)
custom_metric_combo(default_test, truth=default, estimate=.pred_class)
```

Very similar (although not identical) results in this case.

## Workflows & Tuning

More refined workflow and hyperparameter tuning in search of improved prediction.

### Decision Tree model

Use same data set for comparison.

Familiar steps but with a twist: after recipe, use workflow to consolidate process

1. Split data into training / test
2. Model specification
3. Feature engineering (recipe)
4. Workflow combo
5. Mon

#### Split

```{r}
set.seed(2021) ## do we need this? or only set.seed once at top?
cc_split_again <- initial_split(ccdata, prop=0.75, strata=default)
cc_split_tr <- cc_split_again %>% training()
cc_split_te <- cc_split_again %>% testing()
```

#### Spec model

```{r}
cc_dt <- decision_tree() %>% set_engine('rpart') %>% set_mode('classification')

```

#### Recipe 

```{r}
cc_rec <- recipe(default ~ ., data=cc_split_tr) %>%
  step_corr(all_numeric(), threshold=0.85) %>%
  step_normalize(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) 
cc_rec

```

#### Workflow

```{r}
cc_workflow <- workflow() %>%
  add_model(cc_dt) %>%
  add_recipe(cc_rec)
```

#### Fit

```{r}
cc_workflow_fit <- cc_workflow %>% last_fit(split=cc_split)
```

#### Evaluate 

```{r}
cc_workflow_fit %>% collect_metrics()

```
### Cross-Validation!

Splitting data into training set and test set is fundamental to avoiding overfitting.

Downside is that allows only one estimate of model performance, based on the one combination of training / test set.

Enter cross-validation:

* enables you to iterate over a number of training splits to fit the model and the evaluate against a remaining split.
    + you are still doing initial split of training vs test set but WITHIN training set you then do further split, fit these independently, evaluate performance against one of the training splits.
    + so if you have 5 splits within training set, you iterate through 5 times, each time fitting a model to 4 of the training splits and evaluating against remaining training split. (in that way, you cover all permutations) 
* average of results from different iterations provides more accurate sense of the actual accuracy of the fitted model.
* K-fold validation to compare different model types. NOT to fit a final model.
* Once best model type is identified, do a fit on FULL training set and then evaluate on test set.

Based on results, select model type and then fit/predict from there.

#### Create cross-validation folds

```{r}
set.seed(528)
cc_train_folds <- vfold_cv(cc_training, ## specify training data
                           v=5, ## specify number of folds
                           strata=default)
cc_train_folds
```

#### Set custom metrics

```{r}
cc_metrics <- metric_set(roc_auc, sens, spec)
```

#### Fit resamples

Using previous decision tree workflow

```{r}
cc_dt_train_rs <- cc_workflow %>%
  fit_resamples(resamples=cc_train_folds,
                metrics=cc_metrics)

```

#### Check performance metrics

```{r}
cc_dt_train_rs %>% collect_metrics()
```

### Compare Models with Cross-Validation

Use cross-validation to compare overall performance of two different models, in order to decide which one to use/further refine.

#### 1. Set up new model

```{r}
cc_log_reg_model <- logistic_reg() %>%
  set_engine('glm') %>%
  set_mode('classification')

```

#### 2. Set up workflow with model and recipe

* use previous recipe (?)
    + will this work, though? because data source is different
    + or is that ok, as long as similar training data from same original set, with same variables?

```{r}
cc_workflow_logr <- workflow() %>%
  add_model(cc_log_reg_model) %>%
  add_recipe(cc_logreg_recipe)
```

#### 3. Fit resamples

```{r}
cc_log_reg_model_rs <- cc_workflow_logr %>%
  fit_resamples(resample=cc_train_folds,
                metrics=cc_metrics)
```

#### 4. Evaluate

```{r}
cc_log_reg_model_rs %>% collect_metrics
```

#### 5. Evaluate both

* Decision tree model

```{r}
cc_dt_results <- cc_dt_train_rs %>%
  collect_metrics(summarize=FALSE)

cc_dt_results %>%
  group_by(.metric) %>%
  summarize(
    min=min(.estimate),
    med=median(.estimate),
    max=max(.estimate)
  )

```

* Logistic regression model

```{r}
cc_log_results <- cc_log_reg_model_rs %>%
  collect_metrics(summarize=FALSE)

cc_log_results %>%
  group_by(.metric) %>%
  summarize(
    min=min(.estimate),
    med=median(.estimate),
    max=max(.estimate)
  )

```
Analysis:

* Logistic regression has much higher ROC AUC
* Decision tree has better sensitivity, and logistic regression has wider spread, indicating less stability on this metric
* Specificity is essentially equal

Choice of model type depends on what is most important criteria:

* Logistic regression for overall accuracy
* Decision tree if sensitivity is primary: may sacrifice some overall ROC for the sake of higher likelihood of catching true positives (lower false positives) and less concern about true negatives (although very slight difference in this case)

### Hyperparameter Tuning

For decision trees - 3 parameters:

* cost_complexity: penalizes large number of nodes (default: 0.01)
* tree_depth: max length from top to bottom (default: 30)
* min_n: min number of nodes for splitting (default: 20)

Hyperparameter tuning using cross-validation to find optimal values, rather than just using defaults.

Grid search is most common method.

* produces grid of combinations
* random generation provides best chance of discovering optimal

#### Update model to incl tuning

```{r}
cc_dt_ht_model <- decision_tree(cost_complexity=tune(),
                                tree_depth = tune(),
                                min_n=tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')

cc_dt_ht_wkflw <- cc_workflow %>%
  update_model(cc_dt_ht_model)

cc_dt_ht_wkflw
```

#### Grid Search setup

Hyperparameter tuning with grid search

```{r}
set.seed(111)
dt_grid <- grid_random(parameters(cc_dt_ht_model), size=5)

dt_tuning <- cc_dt_ht_wkflw %>%
  tune_grid(
    resamples=cc_train_folds,
    grid=dt_grid,
    metrics=cc_metrics
  )

## get average results for grid combos
dt_tuning %>% collect_metrics()
```
#### Evaluate

```{r}
## show details for each fold and grid 
dt_tuning_results <- dt_tuning %>% collect_metrics(summarize=FALSE)
dt_tuning_results

## summarize by roc_auc results
dt_tuning_results %>% filter(.metric=='roc_auc') %>% group_by(id) %>%
  summarize(min_roc_auc=min(.estimate),
            med_roc_auc=median(.estimate),
            max_roc_auc=max(.estimate)
            )
```

Looks like either **Fold 4** (highest median but some spread) OR **Fold 2** yield best results.

#### Finalize Work Flows

More options for identifying best model from **training** data.

```{r}
## show n best models
dt_tuning %>% show_best(.metric='roc_auc', n=3)
```
Select best model and finalize workflow

```{r}
dt_tuning_best <- dt_tuning %>% select_best(metric='roc_auc') ## roc_auc is default

cc_dt_ht_wkflw_final <- cc_dt_ht_wkflw %>%
  finalize_workflow(dt_tuning_best)

cc_dt_ht_wkflw_final
```

### Apply Model to Test Data

Apply best training model from cross-validation workflow to test data in order to evaluate performance on new data.

```{r}
cc_dt_final_fit <- cc_dt_ht_wkflw_final %>%
  last_fit(split=cc_split)

cc_dt_final_fit %>% collect_metrics()

cc_dt_final_fit %>% collect_predictions()
```

### ROC Curve

```{r}
cc_dt_final_fit %>% collect_predictions() %>%
  roc_curve(truth=default, .pred_Yes) %>%
  autoplot()
```

